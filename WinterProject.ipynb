{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUTJcL_xgeoG",
        "outputId": "12cfd556-53a7-41bf-d440-b4f5bd27dc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/CAIS\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/CAIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw5K8GvS32ae"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import csv\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "EMBEDDINGS_DIR = './glove.6B.50d.txt'\n",
        "CSV_DIR = './data.csv'\n",
        "\n",
        "tweets = []\n",
        "labels = []\n",
        "first = True\n",
        "\n",
        "import csv\n",
        "with open(CSV_DIR, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "      if not first:\n",
        "        labels.append(row[0])\n",
        "        tweets.append(row[2])\n",
        "      else:\n",
        "        first = False\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmqnMD_QtvnV"
      },
      "outputs": [],
      "source": [
        "labels = np.array(labels)\n",
        "tweets = np.array(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taken from RNN lesson\n"
      ],
      "metadata": {
        "id": "4KmuW129ThrU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwy8uDqZKbXY"
      },
      "outputs": [],
      "source": [
        "#TODO: Shuffle data \n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "training_data_init, training_labels_init = shuffle(tweets, labels, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4etmFA3LIcYs"
      },
      "outputs": [],
      "source": [
        "#only use a certain percent of them\n",
        "PERCENT_DATA_USED = .003\n",
        "endIdx = PERCENT_DATA_USED * len(training_data_init)\n",
        "endIdx = int(endIdx)\n",
        "training_data = training_data_init[:endIdx]\n",
        "\n",
        "endIdx = PERCENT_DATA_USED * len(training_labels_init)\n",
        "endIdx = int(endIdx)\n",
        "trainingLabels = training_labels_init[:endIdx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzULJfLcgvhy",
        "outputId": "e6344533-a400-4b5d-ed1e-4593bf6d76ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 -- Tokenizing the tweets: converting sentences to sequence of words\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the tweets (convert sentence to sequence of words)\n",
        "print(\"2 -- Tokenizing the tweets: converting sentences to sequence of words\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(training_data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(training_data)\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg6FCU53h3cl",
        "outputId": "bc9e6613-1a37-4969-e7ca-23ff136a086a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 -- Padding sequences to ensure samples are the same size\n"
          ]
        }
      ],
      "source": [
        "# Pad sequences to ensure samples are the same size\n",
        "print(\"3 -- Padding sequences to ensure samples are the same size\")\n",
        "training_data = pad_sequences(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD6cA8lRh-pm",
        "outputId": "4a678e87-adfc-45de-90cc-24b1d88c35ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 -- Loading pre-trained word embeddings. This may take a few minutes.\n"
          ]
        }
      ],
      "source": [
        "print(\"4 -- Loading pre-trained word embeddings. This may take a few minutes.\")\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(EMBEDDINGS_DIR,'rb')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0].decode('UTF-8')\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YlJRb0cXJ3",
        "outputId": "2a2eda65-e706-4524-9476-72bed6b0defd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 -- Finding word embeddings for words in our tweets.\n"
          ]
        }
      ],
      "source": [
        "print(\"5 -- Finding word embeddings for words in our tweets.\")\n",
        "# prepare word embedding matrix\n",
        "num_words = len(word_index)+1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZwclOwwiGF6",
        "outputId": "27a676bd-2a36-4444-dc89-562b9f00ce2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size:  (4800, 34)\n",
            "Number of Tweets:  4800\n",
            "Max Tweet Length:  34\n",
            "\n",
            "34\n",
            "[   0    0    0    0    0    0    0    0    0 3472 1646 1154   10 1344\n",
            "    8   18 3473 2217    2 3474  103   65  115 2218 1647   11    3  227\n",
            "  579    9   67  254  639   13]\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Data Size: \", training_data.shape)\n",
        "print(\"Number of Tweets: \", training_data.shape[0])\n",
        "print(\"Max Tweet Length: \", training_data.shape[1])\n",
        "print()\n",
        "# print(\"Labels Size: \", labels.shape)\n",
        "print(training_data.shape[1])\n",
        "print(training_data[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfnhmgrPji6t"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Dense, Activation, Flatten\n",
        "from keras.layers import Dropout, concatenate\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from keras import metrics\n",
        "from keras.models import Model\n",
        "import pickle\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPpBDUC_lHix",
        "outputId": "6ddf40f0-3e39-480e-f973-76fd9f8ca562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "trainingLabels2 = []\n",
        "for i in trainingLabels:\n",
        "  if i == '0':\n",
        "    trainingLabels2.append(0)\n",
        "  else:\n",
        "    trainingLabels2.append(1)\n",
        "\n",
        "print(trainingLabels2[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqN0CFPwAVlj"
      },
      "outputs": [],
      "source": [
        "training_data = np.array(training_data)\n",
        "trainingLabels2 = np.array(trainingLabels2)\n",
        "# print(type(trainingLabels[1]))\n",
        "# print(trainingLabels[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCYzyB1yju-_",
        "outputId": "b90b601c-8dbb-494e-fae7-13e244f9b040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_22 (Embedding)    (None, 34, 50)            571400    \n",
            "                                                                 \n",
            " lstm_44 (LSTM)              (None, 34, 50)            20200     \n",
            "                                                                 \n",
            " lstm_45 (LSTM)              (None, 64)                29440     \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 621,105\n",
            "Trainable params: 49,705\n",
            "Non-trainable params: 571,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                    EMBEDDING_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=training_data.shape[1],\n",
        "                    trainable=False))\n",
        "\n",
        "model.add(LSTM(EMBEDDING_DIM,recurrent_dropout=0,activation='relu',return_sequences=True))\n",
        "model.add(LSTM(64,dropout=.1,recurrent_dropout=.1,activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "LOSS = 'binary_crossentropy'\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assisted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "from sklearn.model_selection import train_test_split\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "X_train, X_test, label_train, label_test = train_test_split(training_data, trainingLabels2, test_size=0.33, random_state=seed)\n"
      ],
      "metadata": {
        "id": "6Fo_QkBCSJo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcOR3LBaoAXR",
        "outputId": "4637bc2d-5139-4fb8-e5fa-7c126da430f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "26/26 [==============================] - 20s 555ms/step - loss: 0.6901 - binary_accuracy: 0.5354 - val_loss: 0.6834 - val_binary_accuracy: 0.5914\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - 14s 522ms/step - loss: 0.6745 - binary_accuracy: 0.6007 - val_loss: 0.6599 - val_binary_accuracy: 0.6175\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - 13s 521ms/step - loss: 0.6654 - binary_accuracy: 0.6076 - val_loss: 0.6665 - val_binary_accuracy: 0.5852\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - 13s 512ms/step - loss: 0.6404 - binary_accuracy: 0.6598 - val_loss: 0.6479 - val_binary_accuracy: 0.6437\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 14s 542ms/step - loss: 0.6264 - binary_accuracy: 0.6766 - val_loss: 0.6231 - val_binary_accuracy: 0.6629\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 13s 516ms/step - loss: 0.5976 - binary_accuracy: 0.7009 - val_loss: 0.6176 - val_binary_accuracy: 0.6704\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 14s 524ms/step - loss: 0.5712 - binary_accuracy: 0.7257 - val_loss: 0.6124 - val_binary_accuracy: 0.6648\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 13s 520ms/step - loss: 0.5579 - binary_accuracy: 0.7233 - val_loss: 0.6148 - val_binary_accuracy: 0.6735\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 14s 529ms/step - loss: 0.5519 - binary_accuracy: 0.7295 - val_loss: 0.6108 - val_binary_accuracy: 0.6729\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 14s 523ms/step - loss: 0.5339 - binary_accuracy: 0.7475 - val_loss: 0.6159 - val_binary_accuracy: 0.6791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac9ab3ad90>"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ],
      "source": [
        "TEST_SIZE = 0.5\n",
        "\n",
        "#####################################\n",
        "# TODO: pick number of epochs and batch size\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#####################################\n",
        "\n",
        "model.fit(X_train, label_train, epochs = EPOCHS, batch_size = BATCH_SIZE, validation_split =TEST_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ9_ps5YoBBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cd91a5-b599-46e7-ba94-4617f8747031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 37ms/step - loss: 0.6074 - binary_accuracy: 0.6881\n",
            "Final test cost/loss:  0.607373833656311\n",
            "Final test accuracy:  0.6881313323974609\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(X_test,label_test,batch_size=128)\n",
        "\n",
        "\n",
        "print(\"Final test cost/loss: \", results[0])\n",
        "print(\"Final test accuracy: \",  results[1])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WinterProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}